\documentclass[letter,scriptaddress,twocolumn, prl,showkeys]{revtex4-1}

    \usepackage{microtype}
    \usepackage{blindtext}

	\usepackage{amsmath}%,amssymb} 
	\usepackage{makeidx}
	\usepackage{amsfonts}
	\usepackage[ansinew]{inputenc}
	\usepackage[usenames,dvipsnames]{pstricks}
	\usepackage{subfigure}
	\usepackage{epsfig}
%	\usepackage{pst-grad} % For gradients
	\usepackage{pst-plot} % For axes
	\usepackage[colorlinks,hyperindex]{hyperref}
	\hypersetup
	{
		colorlinks,%
		citecolor=black,%
		linkcolor=black,%
		urlcolor=black,%
	}

%--- Theorem like environments ----
	\newtheorem{theorem}{Theorem}
	\newtheorem{corolary}{Corolary}
	\newtheorem{prop}{Proposition}
	\newtheorem{definition}{Definition}
	\newtheorem{example}{Example}
	\newtheorem{exercise}{Exercise}
	\newtheorem{lemma}{Lemma}

%--- Definindo algumas frescuras.
%\numberwithin{equation}{subsection}
	%\numberwithin{equation}{section}

    \newcommand{\ix}[1]{_\mathrm{#1}}
    \newcommand{\Ix}[1]{^\mathrm{#1}}

    \newcommand{\param}[1]{\texttt{#1}}
    \newcommand{\feedback}{\param{feedback}}
    \newcommand{\gain}{\param{gain}}
    \newcommand{\size}{N}
    \newcommand{\ERprob}{p}
    \newcommand{\RINGk}{k}
    \newcommand{\FFlayers}{l}

	\setlength\textheight{24.5cm}
	
\makeindex

%--------------------------------------------------------
\begin{document}

\title{Optimizing Structural Properties Of Neural Networks With Genetic Algorithms}

\author{T. Staudt, E. Schultheis}
\email{thomas.staudt@stud.uni-goettingen.de, erik.schultheis@uni-goettingen.de}
\affiliation{University of Göttingen}

\date{today}

\begin{abstract}
    In this article we examine how structural properties of neural
    networks, like the size or local synapse densities, affect their
    learning success for various tasks. To do so, we look at a rate based
    model for neural networks and apply the FORCE rule for the learning
    process. A sophisticated matching algorithm
    allows us to quantify the learning success of a network so that the
    fitness of structural characteristics, expressed by integer or
    floating-point parameters, can be evaluated. We then use evolutionary
    optimization methods on these parameters to show that (1) ring
    topologies are generally more successful than strictly random
    topologies, (2) *FAT
    LIST OF INCREDIBLE RESULTS THAT CHANGE THE UNIVERSE*
\end{abstract}

\keywords{genetic algorithms, evolutionary algorithms, FORCE learning, rate networks}

\maketitle

\section{Introduction}
Looking at the field of artificial neuronal networks, one finds a great
variety of different models describing both the dynamic of networks as well
as their plasticities, i.e.\ their learning behavior.
But what exactly makes artificial neuronal networks learn successfully? And
why are certain types of networks and certain learning rules effective for
some problems, but fail at solving others? Confronting and eventually
answering these questions is critical when trying to gain a deep
understanding of neural networks and artificial intelligence. 

Using the above questions as a guideline, we decided to analyze what
network characteristics make the FORCE algorithm, introduced in
\citep{FORCE}, especially suitable for learning periodic patterns.  More
precisely, we wanted to identify which properties of a network are
important for its learning success, and how to choose their
values for an optimal result.  To this end, one option would be to take an
initial network and an adequate periodic pattern to be learned, and
optimize the network's internals for this very task. Then patterns may
emerge that hint at how an optimal network design looks. 

However, we took a different approach:
Instead of fine tuning single internal weights, we were concerned with the
general rules of the network's structure. So we wanted our networks to
be random to a certain degree, which seems biologically plausible, but we
also wanted them to follow certain patterns and allow for specialization:
How dense are the synapses distributed? Is there a strong compartmentalization?
Are recurrent structures frequent? How strong should the feedback signal
be, and how strong should the synapses fire on average?

To address these issues we looked at parameterized random networks, or in
the language of genetics, at different \emph{network genotypes}. These
genotypes contain parametric information about the networks
(\emph{phenotypes}) to be created. 
Different phenotypes with the same genotype may thus have strong
quantitative differences, but they share structural qualities whose fitness
for a preassigned range of tasks we wanted to analyze and improve.

Since calculating the fitness of a genotype required the evaluation of many
different phenotypes and because of the resulting stochastic nature of the
problems we faced, we decided to use genetic algorithms for the
optimization process. 

We first lay out the main aspects of our model in a 
%partially abstract fashion in the next section.
rather high-level overview. 
More details on the actual implementation are then provided in the third
section (or ultimately in the published source
code / supplementary material).
Finally, some of the obtained results are presented and discussed.

\section{Concepts and Model}
\label{sec:concepts}
\paragraph{Network Model} 
%MAYBE WE SHOULD MOTIVATE FORCE HERE, WHICH THEN NATURALLY LEADS TO THE CHOICE OF NETWORK BELOW.

We chose the same rate based network architecture
that is used in the original FORCE publication \citep{FORCE} (denoted as
architecture A) and also in \citep{RM}, where our notation mainly stems
from. So we look at a network $\mathcal{N}$ with $N$ internal neurons and
states $x_i$ for $1 \le i \le N$ that loosely represent the neurons'
membrane potential. The firing rate $r_i$ of the $i$-th internal neuron is
given by $r_i = \tanh x_i$.  Furthermore, the network contains $R$ readout
neurons with states $z_i$ for $1 \le i \le R$, where
\begin{equation} 
    z_i = \sum_{j=1}^{N}\omega\Ix{read}_{ij}\,r_j
\end{equation}
with the readout weights $\omega\Ix{read}$.
Taking into account both internal dynamics as well as an external feedback
pathway, the dynamics $t \mapsto x_i(t)$ of the single neurons are governed
by \footnote{~Note that we did not include external input signals in this
formula, as is done in \citep{FORCE} and \citep{RM}. We in fact enabled
inputs in our code base, but did not use them for the main results.}
\begin{equation}
    \dot{x}_i(t) = -x_i(t) + \sum_{i=1}^N \omega\Ix{rec}_{ij}\, r_j(t) + 
                   \sum_{i=1}^R \omega\Ix{fb}_{ij}\, z_j(t)~,
\end{equation}
where we introduce the internal recurrent synapse weights
$\omega\Ix{rec}_{ij}$ and the feedback synapse weights
$\omega\Ix{fb}_{ij}$. One can see that the neurons exponentially
decay if they are not presented stimulus by other neurons. 
We solved this system of differential equations by
a simple Newton method with integration time steps $dt = 0.01$, which was
also used in \citep{FORCE}. 
%(How we constructed $\omega\Ix{rec}$ and
%$\omega\Ix{fb}$ will be described in \emph{Parametric Random Networks})


\paragraph{Learning} Learning took place by stepwisely adapting the weights
$\omega\Ix{read}$ so that the dynamics $t\mapsto r_i(t)$ of the readout
neurons should eventually match a given periodic target pattern $t\mapsto
r'_i(t)$, called a \emph{task} from now on, as accurately as possible. To
update $\omega\Ix{read}$ we chose a variant of the FORCE algorithm
\citep{FORCE} that rapidly modifies the feedback loop to keep the errors
$|r_i(t) - r'_i(t)|$ small from the beginning on.

While the authors of \citep{FORCE} update the weights $\lambda\Ix{read}$ in
fixed intervals (every second time step), we used an adaptive mechanism to
determine how often learning should occur. This was initially introduced
for performance reasons, as the single learning steps proved to be the main
computational bottleneck, but we also found that the long time network
dynamics tended to be slightly more accurate for adaptive learning steps.

\paragraph{Network Genotypes} Since we are not
interested in single network properties but rather want to find out which
structural elements of networks are important for learning success, we
introduced  \emph{network genotypes}.
Such a genotype $G$ consists of a set $\Theta$ of parameters that determine
which concrete networks are created when $G$ is expressed in a single
phenotype. Technically $G$ may be understood as a stochastic network
generator that returns a network $\mathcal{N}=G(\lambda)$ for every seed
value $\lambda$.
The genotypes' parameters $\theta\in\Theta$ were either integer or floating
point values and can be classified as affecting either (1) the topology of
$\omega\Ix{rec}$ or (2) the weight values of $\omega\Ix{rec}$ and
$\omega\Ix{fb}$. Important examples for (1) are the networks size $N$, the
occupation probability $p$ (for random Erdös-Renyi networks), the
neighborhood range $k$ (for ring topologies), or the number $l$ of layers
(for feed-forward networks). We furthermore introduced ratio parameters
that allowed us to interpolate between different topologies. Optimized
parameters of type (2) were the feedback strength $\feedback$ (with
$\omega\Ix{fb}\propto \feedback$) and the gain value $\gain$ (with
$\omega\Ix{rec} \propto \gain$).


\paragraph{Fitness} The next step was to define the \emph{fitness} of a given
genotype when confronted with a diverse range of tasks, like waves of
different frequencies. To express this formally, we introduced
\emph{challenges}: For a given seed value $\lambda$, the challenge $C$
yields a task $r' = C(\lambda)$ according to some (parametrized)
distribution. If we now make use of a success function $S(\mathcal{N},
r')\in\{0, 1\}$ that decides, if the network $\mathcal{N}$
has successfully reproduced the desired output $r'$, the fitness $F_C(G)$
of a genotype $G$ for a challenge $C$ may reasonably be defined as the
expectation value of $\lambda \mapsto S(G(\lambda), C(\lambda))$. For fixed
challenges, the main task of optimizing the genotype is now reduced to
finding parameters $\Theta$ that maximize $F_C$.


% IDEA: Maybe put this part at the end of the paper. Most things needed to
% understand the results (in principle) have already been mentioned at this
% point.
\section{Methods and Implementation}
\label{sec:implementation}

\paragraph{Tasks}
For the optimization to work properly, the tasks presented to the networks
must be reasonably chosen; they must be learnable by FORCE (at least in
principle) and must be compatible with technical aspects like the
integration time step.
%have (for the most part) to be solvable by FORCE. 
While it was shown in \citep{FORCE} that large recurrent neural networks are
capable of learning very complex tasks, the optimization requires an
extensive number of learning processes, since every single evaluation of
the fitness $F_C$ needs dozens of tested phenotypes (see below). Therefore,
the networks had to be restricted to being quite small ($\approx 100$
neurons) and are consequently less powerful. 

To get a first estimate whether it might be possible to learn a given task
using FORCE with 100 neurons in our network model, we looked at a single
sinusoidal of frequency $\omega$ and determined the frequency range in
which an (unoptimized) network (i.e. \gain, \feedback, \ERprob) is able to
learn the function (see fig. \ref{}). 

Since we expect/hope that the optimized network extends this range of
possible frequencies, we choose the frequencies for the optimization tasks
from the slightly larger interval $[0.6e-2, 4]$ such that their logarithms
are uniformly spaced. This allows for good resolution for both high and low
frequencies.


\paragraph{Success}
As mentioned in \emph{Concepts and Methods}, we need a function $S$
that decides weather a network succeeded or failed to solve a task.
To this end
%In order to optimize the function reconstruction of a force network,
a measure $f(r', r)$ for the concordance of the target function $r'(t)$ and
the network readout $r(t)$ is sought. Though one could use a simple
difference based metric, like the norm $f\ix{2}(r, r')
= \left\|r-r'\right\|_2$, this has the drawback that small phase
mismatches could produce high differences, even though the
shape of $r$ matches the one of $r'$ well.

Instead, a measure that is time-shift independent and maximized for functions of
identical shape is the maximum of the cross correlation 
\begin{align}
    f\ix{cor}(r, r') = \max_t \frac{\left<r, r'(\cdot
    - t)\right>}{\sqrt{\left< r, r\right> \left<r', r' \right>}}~.
\end{align}
Algorithmically, the signals $r$ and $r'$ are split into overlapping chunks
which are correlated separately.  This saves computational power and
prevents minute differences in frequency to accumulate into a huge phase
shift, so the phase only has to remain relatively constant over the
timescale of a single chunk (which is about 10 seconds of simulated time).
This measure becomes problematic for functions that are very close to zero
for a longer time (order of chunk size), but since we are going to work
with superpositions of sinusoidals of different frequencies, this drawback
is unproblematic.

Using this measure of quality, the success function for a network
$\mathcal{N}$ with readout $r$ for the task $r'$ was taken to be 
\begin{equation}
    S(\mathcal{N}, r') = \begin{cases} 1\quad\mathrm{if}~f\ix{cor}(r, r') \ge 0.95,\\
                                       0\quad\mathrm{if}~f\ix{cor}(r, r') < 0.95~.
                         \end{cases}
\end{equation}
The threshold value of 0.95 was determined empirically, as signals $r$ with
$f(r, r') \ge 0.95$ met our assessment of what we considered a good
reproduction of the target pattern.


\paragraph{Genetic Optimization}
Due to the stochastic nature of the fitness function $F_C(G)$ of a genotype
$G$, a very high number of samples would be required to obtain sufficiently
smooth approximations of $F_C(G)$ for typical optimization methods like
gradient descend or simulated annealing. But as the computational cost for
even one learning process was considerable high, these methods were not
feasible.

%and the fact that single evaluations of $F_C$ are
%computationally very expensive, thus preventing increasing the number of
%samples such that the estimation $E(F_C)$ becomes sufficiently smooth,
%typical optimization methods like gradient descent or simulated annealing
%are not usable.

An alternative approach would be to use techniques from Monte Carlo
integration, e.g.  stratified or importance sampling, to aid in the
evaluation of $F_C(G)$, but since $G(\lambda)$ and $C(\lambda)$ depend
non-continuously on the seed value $\lambda$, this too would not solve the
problem.

Therefore, we optimized $F_C(G)$ using a genetic algorithm, which is inherently
capable of coping with the stochastic fitness function. In each iteration
of the algorithm, a generation $\mathcal{G}_t = \left\{ G_i~|~1\le i\le N\right\}$
of $N$ individuals (Generators) is converted into a new one, $\mathcal{G}_{t+1}$,
according to the following scheme:
\begin{description}
 \item[Evaluation] The fitness $F_i = F_C(G_i)$ of each individual is
     measured using at least 20 samples (and adaptively more if the
     $(F_i)_{1\le i\le N}$ are spaced closely, to at most 100).
 \item[Selection] All individuals are grouped into disjunct pairs for which
     we check which one has the higher fitness. This is repeated $k=10$ times,
     counting the number of wins for each individual. Based thereon we use the
     $p=50\%$ best individuals $\mathcal{G}\Ix{survive}_t$ to build the next
     generation.  By using pair comparisons instead of simple ordering
     according to the fitness, we allow weaker individuals to reach
     the next generation (albeit with low probability), keeping the gene
     pool more diverse. 

\item[Reproduction] We obtain the next generation $\mathcal{G}_{t+1}$ by first
    generating $2N$ candidates via \emph{mutating} or \emph{recombining}
    elements of $\mathcal{G}\Ix{survive}_t$. This means we either change
    a single parameter randomly or take two individuals and randomly choose
    for each parameter from which parent to take the value.  We then
    perform a single sample fitness test and remove all networks that fail,
    until only $N$ networks are left; these form $\mathcal{G}_{t+1}$. By
    this approach the most unpromising elements of the new generation are
    removed before the expensive fitness evaluation takes place. 
\end{description} 

\paragraph{Choice of Parameters}


\section{Results}
Baseline
gain and feedback tests

(feedback shifts possible frequencies to higher values)

success of omega: erdös-renyi, ring
optimizations: ER topology, Ring Topology, Mixing

\section{Discussion}

\blindtext



\bibliography{sources}

\end{document}
